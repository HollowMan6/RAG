{"result": "(\"entity\"<|>CSC<|>ORGANIZATION<|>CSC provides supercomputing resources and guides on working with large language models (LLMs) on their supercomputers)\n##\n(\"entity\"<|>GPU<|>ORGANIZATION<|>A processing unit critical for running large language models, especially for tasks like inference and training)\n##\n(\"entity\"<|>PUHTI<|>ORGANIZATION<|>A supercomputer provided by CSC with NVIDIA V100 GPUs, having 32 GB of VRAM)\n##\n(\"entity\"<|>MAHTI<|>ORGANIZATION<|>A supercomputer provided by CSC with NVIDIA A100 GPUs, having 40 GB of VRAM)\n##\n(\"entity\"<|>LUMI<|>ORGANIZATION<|>A supercomputer provided by CSC with AMD MI250x GPUs, having 64 GB of VRAM in a single GCD)\n##\n(\"entity\"<|>ELEUTHERAI<|>ORGANIZATION<|>An organization mentioned for their Transformer Math 101 blog post, providing insights into the memory requirements for training LLMs)\n##\n(\"entity\"<|>HUGGING FACE<|>ORGANIZATION<|>A platform providing libraries for machine learning, specifically mentioned for its HF Trainer for fine-tuning models)\n##\n(\"entity\"<|>IMDB MOVIE REVIEW DATASET<|>EVENT<|>A dataset used in an example for fine-tuning a model, demonstrating the technical task of model training)\n##\n(\"relationship\"<|>CSC<|>PUHTI<|>CSC provides the Puhti supercomputer as a resource for computing<|>9)\n##\n(\"relationship\"<|>CSC<|>MAHTI<|>CSC provides the Mahti supercomputer as a resource for computing<|>9)\n##\n(\"relationship\"<|>CSC<|>LUMI<|>CSC provides the LUMI supercomputer as a resource for computing<|>9)\n##\n(\"relationship\"<|>PUHTI<|>GPU<|>Puhti uses NVIDIA V100 GPUs for processing<|>8)\n##\n(\"relationship\"<|>MAHTI<|>GPU<|>Mahti uses NVIDIA A100 GPUs for processing<|>8)\n##\n(\"relationship\"<|>LUMI<|>GPU<|>LUMI uses AMD MI250x GPUs for processing<|>8)\n##\n(\"relationship\"<|>HUGGING FACE<|>IMDB MOVIE REVIEW DATASET<|>Hugging Face libraries used for fine-tuning a model with the IMDb movie review dataset<|>7)\n<|COMPLETE|>", "input": "\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis's Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n(\"entity\"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n(\"entity\"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n(\"entity\"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis's money supply)\n##\n(\"relationship\"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal's (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation's debut on the public markets isn't indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n(\"entity\"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n(\"entity\"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n(\"relationship\"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad's capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia's capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia's Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n(\"entity\"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n(\"entity\"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n(\"entity\"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n(\"entity\"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n(\"entity\"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n(\"entity\"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n(\"entity\"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia's Alhamia Prison)\n##\n(\"entity\"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n(\"entity\"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n(\"entity\"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n(\"relationship\"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n(\"relationship\"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n(\"relationship\"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n(\"relationship\"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n(\"relationship\"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n(\"relationship\"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: # Working with large language models on supercomputers\n\nThis guide gives some examples and hints on how to work with large\nlanguage models (LLMs) on CSC's supercomputers.\n\n## LLMs and GPU memory\n\nIf you are doing inference (using a model, rather than training), you\ncan in some cases do without a GPU, for example if the model is small\nenough or has been reduced by quantization. In most other cases you\nwill need to use a GPU.\n\nIn order to use an LLM (or any neural network) with a GPU, the model\nneeds to be loaded into the GPU memory (VRAM). LLMs can be very large\nand here the size of the GPU memory becomes critical. You can refer to\n[our table of GPU stats](gpu-ml.md#puhti-mahti-or-lumi) for the full\ndetails, but our GPUs have VRAM memory as follows:\n\n- 32 GB on Puhti (NVIDIA V100)\n- 40 GB on Mahti (NVIDIA A100)\n- 64 GB on LUMI (single GCD of an AMD MI250x)\n\nThe model size is the number of parameters times 2 bytes (for 16 bit\nweights) or times 4 bytes (for 32 bit). For example a 30 billion\nparameter model with fp16 takes up 60 GB of memory. In practice [for\ninference there's up to 20% overhead][1] so you might actually need\naround 70 GB of memory, and thus even a single GCD in LUMI might not\nbe enough for our example model.\n\nFor training a lot more memory is needed as not only the model, but\nalso the optimizer states, gradients and activations need to be\nstored. As a *very* rough estimate, around 4-6x the model size (in GB)\nis needed for fine-tuning a model, but this depends a lot on the\ndetails. So for our example 30B parameter fp16 model, it might require\n60 GB x 6 = 360 GB of GPU memory for training! We'll discuss ways to\nsolve this problem in the sections below. See the [Transformer Math\n101 blog post by EleutherAI for more details][1].\n\n## Fine-tuning LLMs\n\nWe have a [git repository with some example scripts for doing LLM\nfine-tuning on Puhti or Mahti][2]. The example uses the [Hugging Face\n(HF) libraries][3] and in particular the HF Trainer to train a given\nmodel (taken from the HF model repositories) with the IMDb movie\nreview dataset. The task itself might not make much sense, it's just\nused to demonstrate the technical task of fine-tuning a model with a\ngiven dataset.\n\nThe examples, by default, use the [EleutherAI/gpt-neo-1.3B][4] model,\nas it will fit into the memory of a single GPU in Puhti. Given that\nit's a 1.37 billion parameter model with 32 bit weights, according to\nour rule-of-thumb, mentioned above, it might require up to 1.37x4x6 =\n32 GB of memory for training, so it should just fit into the 32 GB\nmaximum of Puhti's V100 (if we're lucky).\n\nThe repository has basic launch scripts for Puhti and Mahti for 1 GPU,\n4 GPUs (a full node) and 8 GPUs (two full nodes). The Slurm scripts\nare essentially the same as for any PyTorch DDP runs, see our\n[Multi-GPU and multi-node ML guide](ml-multi.md#pytorch-ddp) for\nexamples, or just take a look at [the scripts in the GitHub\nrepository][2]:\n\n- [`run-finetuning-puhti-gpu1.sh`](https://github.com/CSCfi/llm-fine-tuning-examples/blob/master/run-finetuning-puhti-gpu1.sh) - fine-tuning on Puhti with 1 GPU\n- [`run-finetuning-puhti-gpu4.sh`](https://github.com/CSCfi/llm-fine-tuning-examples/blob/master/run-finetuning-puhti-gpu4.sh) - fine-tuning on Puhti with one full node (4 GPUs)\n- [`run-finetuning-puhti-gpu8.sh`](https://github.com/CSCfi/llm-fine-tuning-examples/blob/master/run-finetuning-puhti-gpu8.sh) - fine-tuning on Puhti with two full nodes (8 GPUs in total)\n\n(The repository also has scripts for Mahti if you check the file\nlisting.)\n\nThe basic multi-GPU versions are all using PyTorch Distributed Data\nParallel (DDP) mode, in which each GPU has a full copy of the\nmodel. Only the training data is distributed across the different\nGPUs. This means that the full model must fit into a single GPU.\n\nIf your model doesn't fit into a single GPU on Puhti, it might work on\nMahti or LUMI, but first check with the rule-of-thumb calculation\nmentioned above if there's even a chance of that! If not, read on for\nPEFT and FSDP approaches.\n\n### Using PEFT and LoRA\n\nIf your model would fit into the GPU memory, but cannot handle all the\nextra memory needed by the overhead of the fine-tuning process, the\nsolution may be to use the [Parameter Efficient Fine-Tuning (PEFT)][5]\nlibrary which trains a smaller number of extra parameters, which\nreduces the training overhead a substantially. PEFT supports many\nmethods including [Lo\n######################\nOutput:", "parameters": {"model": "gpt-4-turbo-preview", "temperature": 0.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "top_p": 1.0, "max_tokens": 4000, "n": 1}, "history": null}