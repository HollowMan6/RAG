llama_index
beautifulsoup4
markdown
llama-index-embeddings-huggingface
llama-index-llms-llama-cpp
cupy-cuda12x
# CMAKE_ARGS="-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS -DLLAMA_CUDA=on" pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir
# pip install -U "jax[cuda12_pip]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html

## For AMD GPUs on LUMI
# CMAKE_ARGS="-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS -DLLAMA_CLBLAST=on -DLLAMA_HIPBLAS=ON -DCMAKE_C_COMPILER=/appl/lumi/SW/CrayEnv/EB/rocm/5.6.1/llvm/bin/clang -DCMAKE_CXX_COMPILER=/appl/lumi/SW/CrayEnv/EB/rocm/5.6.1/llvm/bin/clang++ -DCMAKE_PREFIX_PATH=/appl/lumi/SW/CrayEnv/EB/rocm/5.6.1" FORCE_CMAKE=1 pip install llama-cpp-python
# pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm5.6/
# git clone https://github.com/google/jax
# cd jax
# git reset --hard jax-v0.4.26
# TEST_TMPDIR=/scratch/project_462000007/sonjiang/RAG-main/jax/.bazelcache python build/build.py --enable_rocm --rocm_path=/appl/lumi/SW/CrayEnv/EB/rocm/5.6.1 --bazel_options=--override_repository=xla=/scratch/project_462000007/sonjiang/RAG-main/jax/xla
# pip install -e .
